{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyou-wbg/meta_processing/blob/main/0604_Deduping_PipelineReady.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load data\n"
      ],
      "metadata": {
        "id": "ePqK7ToMcz3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJGzuWC35r8U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "directory = '/content/drive/MyDrive/finalMetaFile'\n",
        "df = pd.read_csv(os.path.join(directory, 'combined_metadata_final12_features.csv'))\n",
        "df=df[['sourcename', 'title']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing titles for NLP"
      ],
      "metadata": {
        "id": "SVeUaXThdvhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_english(df):\n",
        "  def is_english(text):\n",
        "    words = word_tokenize(text)\n",
        "    english_word_count = sum(len(word) >= 2 for word in words)\n",
        "    return english_word_count / len(words) >= 0.9\n",
        "\n",
        "  df_filtered = df[df['title'].apply(is_english)]\n",
        "  return df_filtered\n",
        "\n",
        "df['title'] = df['title'].astype(str)\n",
        "df = df[~df['title'].str.contains('و|д|я|š|ü|à|é|ś|ä|ú|의|θ|λ|ό|τ|η|τ|α|の|と', regex=True)]\n",
        "df = remove_non_english(df.copy())"
      ],
      "metadata": {
        "id": "3O1IwCPNgd2a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sourcename'] = df['sourcename'].apply(lambda x: x.replace('metadata_', ''))\n",
        "df['sourcename'] = df['sourcename'].apply(lambda x: x.replace('_unifiedColName.csv', ''))\n",
        "df['sourcename'].replace(['url1_ie_table_complete', 'scopus_fixed_encoding','SSRN'], ['3ie', 'scopus','ssrn'], inplace=True)\n"
      ],
      "metadata": {
        "id": "KcV-3flvL4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Get word count for grouping\n",
        "This is an additional featue to run the deduping faster, however it is not a must-step to run."
      ],
      "metadata": {
        "id": "Qyi4q9fg7QJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_words(title):\n",
        "  words = title.split()\n",
        "  return len(words)\n",
        "\n",
        "df['word_count'] = df['title'].apply(count_words)"
      ],
      "metadata": {
        "id": "RDrrIaDy7h1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min=5\n",
        "max=20\n",
        "\n",
        "df = df[(df['word_count'] >= min) & (df['word_count'] <= max)]\n",
        "\n",
        "title_sample_size= 1000000\n",
        "\n",
        "def sample_dataframe(df, sample_size):\n",
        "  if len(df) > sample_size:\n",
        "    return df.sample(n=sample_size, random_state=321)\n",
        "  else:\n",
        "    return df\n",
        "\n",
        "\n",
        "df_title1 = sample_dataframe(df, title_sample_size)\n",
        "df_title1 = df_title1.rename(columns={\"title\": \"title1\",\"sourcename\": \"sourcename1\",\"word_count\": \"word_count1\"})\n",
        "\n",
        "df_title2 = df.drop(df_title1.index) # OR below line\n",
        "#df_title2 = df[~df['title'].isin(df_title1['title1'])]\n",
        "df_title2 = df_title2.rename(columns={\"title\": \"title2\",\"sourcename\": \"sourcename2\",\"word_count\": \"word_count2\"})\n",
        "df_title2=df_title2_2"
      ],
      "metadata": {
        "id": "JQHShGTs-_D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Make pair of titles for deduping"
      ],
      "metadata": {
        "id": "EvqoqwBYe8pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_title1.to_csv(os.path.join(directory, 'df_title1.csv'), index=False)\n",
        "df_title2.to_csv(os.path.join(directory, 'df_title2.csv'), index=False)\n",
        "df_title1 = df_title1.reset_index(drop=True)\n",
        "df_title2 = df_title2.reset_index(drop=True)\n",
        "df_pair=pd.concat([df_title1, df_title2], axis=1)\n",
        "df_pair = df_pair.drop(df_pair.loc[df_pair['sourcename1'] == df_pair['sourcename2']].index)\n",
        "df_pair = df_pair.drop(df_pair.loc[(df_pair['word_count2'] > df_pair['word_count1']+3) | (df_pair['word_count2'] < df_pair['word_count1']-3)].index)\n",
        "df_pair.to_csv(os.path.join(directory, 'df_pair.csv'), index=False)"
      ],
      "metadata": {
        "id": "zxVFDzyD7h_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_dataframe(df, sample_size):\n",
        "  if len(df) > sample_size:\n",
        "    return df.sample(n=sample_size, random_state=321)\n",
        "  else:\n",
        "    return df\n",
        "\n",
        "title_sample_size=2000\n",
        "df_pair = sample_dataframe(df_pair, title_sample_size)\n",
        "df_pair.to_csv(os.path.join(directory, 'df_pair_2k.csv'), index=False)"
      ],
      "metadata": {
        "id": "AVGbbWofBUIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Compute distance"
      ],
      "metadata": {
        "id": "GWVlCcARC6qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import printable\n",
        "from collections import Counter\n",
        "\n",
        "printable_dict = {c:i for i,c in enumerate(printable)}\n",
        "\n",
        "def vectorize(title):\n",
        "    cnt = Counter(title)\n",
        "    array = [0 for _ in printable]+[0]\n",
        "    for c,n in cnt.items():\n",
        "        if c in printable_dict:\n",
        "            array[printable_dict[c]]=n\n",
        "        else:\n",
        "            array[-1]+=n\n",
        "    return np.array(array)\n",
        "\n",
        "vects = []\n",
        "for title in df_pair.title1:\n",
        "    vects.append(vectorize(title))"
      ],
      "metadata": {
        "id": "iCVMb1Y9Bm0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pair['vects1']=vects"
      ],
      "metadata": {
        "id": "y6GCT6YxHN1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vects_2 = []\n",
        "for title in df_pair.title2:\n",
        "    vects_2.append(vectorize(title))\n",
        "df_pair['vects2']=vects_2"
      ],
      "metadata": {
        "id": "t6UNeteoBm7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l1_distance(v1, v2):\n",
        "  # Ensure both arrays have the same size\n",
        "  if len(v1) != len(v2):\n",
        "    raise ValueError(\"Input vectors must have the same size\")\n",
        "  # Calculate absolute difference between corresponding elements\n",
        "  diff = np.abs(np.array(v1) - np.array(v2))\n",
        "  # Return the sum of absolute differences (L1 norm)\n",
        "  return np.sum(diff)\n",
        "\n",
        "# Apply the function to each pair of vectors in the columns\n",
        "df_pair['L1_distance'] = df_pair.apply(lambda row: l1_distance(row['vects1'], row['vects2']), axis=1)\n",
        "df_pair['annotation_label'] = df_pair.apply(lambda row: 1 if row['title1'] == row['title2'] else 0, axis=1)\n",
        "df_pair.to_csv(os.path.join(directory, '0503_df_pair_2k_distance_withL1.csv'), index=False)"
      ],
      "metadata": {
        "id": "bVv-w0ggHtSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lifeay2lgA5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df_pair will contain list of titles that is examined for deduplication. Any item that is labeld as 1 from 'annotation_label' variable indicates duplicated titles."
      ],
      "metadata": {
        "id": "RUtcPshkgBgL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOk8MAQbZjnOAnFcrr/zzAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}